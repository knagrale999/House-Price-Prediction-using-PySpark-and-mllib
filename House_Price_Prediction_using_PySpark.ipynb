{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House Price Prediction using PySpark.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "61_inKEc54ZK"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import udf, col\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_8NEPH_59R8"
      },
      "source": [
        "spark_session = SparkSession.builder.master(\"local[2]\").appName(\"HousingRegression\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY7219L66KoU"
      },
      "source": [
        "spark_context = spark_session.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mkHgq086QNn"
      },
      "source": [
        "spark_sql_context = SQLContext(spark_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a3FhjK36TXy"
      },
      "source": [
        "TRAIN_INPUT = 'train.csv'\n",
        "TEST_INPUT = 'test.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWm2RApa6jQi"
      },
      "source": [
        "pd_train = pd.read_csv(TRAIN_INPUT)\n",
        "pd_test = pd.read_csv(TEST_INPUT)\n",
        "na_cols = pd_train.columns[pd_train.isna().any()].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No8UUWrZ7PzQ"
      },
      "source": [
        " pd_train.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6dek0DT6n81"
      },
      "source": [
        "total = pd_train.isnull().sum().sort_values(ascending=False)\n",
        "percent = (pd_train.isnull().sum()/pd_train.shape[0]).sort_values(ascending=False)\n",
        "\n",
        "missing = pd.concat([total, percent], axis=1, keys=['Total', 'Perc_missing'])\n",
        "missing.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBB6S7DO7LRs"
      },
      "source": [
        "pd_train = pd_train.drop((missing[missing['Perc_missing'] >= 0.15]).index,1)\n",
        "pd_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUnhZwjS7ZNA"
      },
      "source": [
        "pd_train['New'] = pd_train['OverallQual'] * pd_train['GarageArea'] * pd_train['GrLivArea']\n",
        "pd_test['New'] = pd_test['OverallQual'] * pd_test['GarageArea'] * pd_test['GrLivArea']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e-0dyx77r6o"
      },
      "source": [
        "train_cols = list(pd_train.columns)\n",
        "train_cols.remove('SalePrice')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8vqSGO57uWF"
      },
      "source": [
        "pd_test = pd_test[train_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVBf3rHl7xOA"
      },
      "source": [
        "pd_test.columns[pd_test.isna().any()].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqSYXQzQ70G7"
      },
      "source": [
        "\n",
        "for col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n",
        "    pd_train[col] = pd_train[col].fillna(\"None\")\n",
        "    pd_test[col] = pd_test[col].fillna(\"None\")\n",
        "    \n",
        "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
        "    pd_train[col] = pd_train[col].fillna(\"None\")\n",
        "    pd_test[col] = pd_test[col].fillna(\"None\")\n",
        "    \n",
        "for col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n",
        "    pd_train[col] = pd_train[col].fillna(0)\n",
        "    pd_test[col] = pd_test[col].fillna(0)\n",
        "    \n",
        "pd_train['MasVnrType'] = pd_train['MasVnrType'].fillna(\"None\")\n",
        "pd_test['MasVnrType'] = pd_test['MasVnrType'].fillna(\"None\")\n",
        "\n",
        "pd_train['MasVnrArea'] = pd_train['MasVnrArea'].fillna(0)\n",
        "pd_test['MasVnrArea'] = pd_test['MasVnrArea'].fillna(0)\n",
        "\n",
        "pd_train['Electrical'] = pd_train['Electrical'].fillna(pd_train['Electrical'].mode()[0])\n",
        "pd_test['Electrical'] = pd_test['Electrical'].fillna(pd_test['Electrical'].mode()[0])\n",
        "\n",
        "print(pd_train.isnull().sum().max())\n",
        "print(pd_test.isnull().sum().max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0s7VlPI8BqK"
      },
      "source": [
        "cat_columns = pd_train.select_dtypes(include=['object']).columns\n",
        "pd_train[cat_columns] = pd_train[cat_columns].fillna('NoData')\n",
        "pd_test[cat_columns] = pd_test[cat_columns].fillna('NoData')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNOd0U9T8S4K"
      },
      "source": [
        "train_df = spark_session.createDataFrame(pd_train)\n",
        "test_df = spark_session.createDataFrame(pd_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rOMK9CY_37a"
      },
      "source": [
        "train_df = train_df.select([c for c in train_df.columns if c not in na_cols])\n",
        "train_cols = train_df.columns\n",
        "train_cols.remove('SalePrice')\n",
        "test_df = test_df.select(train_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2AqznII_7LG"
      },
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "\n",
        "test_df = test_df.withColumn(\"BsmtFinSF1\", test_df[\"BsmtFinSF1\"].cast(IntegerType()))\n",
        "test_df = test_df.withColumn(\"BsmtFinSF2\", test_df[\"BsmtFinSF2\"].cast(IntegerType()))\n",
        "test_df = test_df.withColumn(\"BsmtUnfSF\", test_df[\"BsmtUnfSF\"].cast(IntegerType()))\n",
        "test_df = test_df.withColumn(\"TotalBsmtSF\", test_df[\"TotalBsmtSF\"].cast(IntegerType()))\n",
        "test_df = test_df.withColumn(\"BsmtFullBath\", test_df[\"BsmtFullBath\"].cast(IntegerType()))\n",
        "test_df = test_df.withColumn(\"BsmtHalfBath\", test_df[\"BsmtHalfBath\"].cast(IntegerType()))\n",
        "test_df = test_df.withColumn(\"GarageCars\", test_df[\"GarageCars\"].cast(IntegerType()))\n",
        "test_df = test_df.withColumn(\"GarageArea\", test_df[\"GarageArea\"].cast(IntegerType()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGzlDGmvAR5R"
      },
      "source": [
        "train_string_columns = []\n",
        "\n",
        "for col, dtype in train_df.dtypes:\n",
        "    if dtype == 'string':\n",
        "        train_string_columns.append(col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-RmnSpVAYhL"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+'_index', handleInvalid='keep').fit(train_df) for column in train_string_columns]\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "train_indexed = pipeline.fit(train_df).transform(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8ynZSWrAZdK"
      },
      "source": [
        "test_string_columns = []\n",
        "\n",
        "for col, dtype in test_df.dtypes:\n",
        "    if dtype == 'string':\n",
        "        test_string_columns.append(col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2rzkmf2Aex6"
      },
      "source": [
        "indexers2 = [StringIndexer(inputCol=column, outputCol=column+'_index', handleInvalid='keep').fit(test_df) for column in test_string_columns]\n",
        "\n",
        "pipeline2 = Pipeline(stages=indexers2)\n",
        "test_indexed = pipeline2.fit(test_df).transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioeZ3HwPAnAy"
      },
      "source": [
        "def get_dtype(df,colname):\n",
        "    return [dtype for name, dtype in df.dtypes if name == colname][0]\n",
        "\n",
        "num_cols_train = []\n",
        "for col in train_indexed.columns:\n",
        "    if get_dtype(train_indexed,col) != 'string':\n",
        "        num_cols_train.append(str(col))\n",
        "        \n",
        "num_cols_test = []\n",
        "for col in test_indexed.columns:\n",
        "    if get_dtype(test_indexed,col) != 'string':\n",
        "        num_cols_test.append(str(col))\n",
        "\n",
        "train_indexed = train_indexed.select(num_cols_train)\n",
        "test_indexed = test_indexed.select(num_cols_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7HBTcyAAsZB"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "vectorAssembler = VectorAssembler(inputCols = train_indexed.drop(\"SalePrice\").columns, outputCol = 'features').setHandleInvalid(\"keep\")\n",
        "\n",
        "train_vector = vectorAssembler.transform(train_indexed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMeEzSE1Ay5f"
      },
      "source": [
        "vectorAssembler2 = VectorAssembler(inputCols = test_indexed.columns, outputCol = 'features').setHandleInvalid(\"keep\")\n",
        "\n",
        "test_vector = vectorAssembler2.transform(test_indexed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS8_kclqA7vP"
      },
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "test_vector = test_vector.withColumn(\"SalePrice\", lit(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ruHAUT4BNZq"
      },
      "source": [
        "splits = train_vector.randomSplit([0.7, 0.3])\n",
        "train = splits[0]\n",
        "val = splits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdRgK36PBcOy"
      },
      "source": [
        "\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol='SalePrice', maxIter=10, \n",
        "                      regParam=0.8, elasticNetParam=0.1) # It is always a good idea to play with hyperparameters.\n",
        "lr_model = lr.fit(train)\n",
        "\n",
        "trainingSummary = lr_model.summary\n",
        "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "print(\"r2: %f\" % trainingSummary.r2)\n",
        "\n",
        "lr_predictions = lr_model.transform(val)\n",
        "lr_predictions.select(\"prediction\",\"SalePrice\",\"features\").show(5)\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
        "                 labelCol=\"SalePrice\",metricName=\"r2\")\n",
        "print(\"R Squared (R2) on val data = %g\" % lr_evaluator.evaluate(lr_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48mB93viBeUT"
      },
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(featuresCol = 'features', labelCol='SalePrice', \n",
        "                           maxDepth=20, \n",
        "                           minInstancesPerNode=2,\n",
        "                           bootstrap=True\n",
        "                          )\n",
        "rf_model = rf.fit(train)\n",
        "\n",
        "rf_predictions = rf_model.transform(val)\n",
        "rf_predictions.select(\"prediction\",\"SalePrice\",\"features\").show(5)\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "rf_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
        "                 labelCol=\"SalePrice\",metricName=\"r2\")\n",
        "print(\"R Squared (R2) on val data = %g\" % rf_evaluator.evaluate(rf_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Cfj8xfCGqQ"
      },
      "source": [
        "rf_predictions2 = rf_model.transform(test_vector)\n",
        "pred = rf_predictions2.select(\"Id\",\"prediction\")\n",
        "pred = pred.withColumnRenamed(\"prediction\",\"SalePrice\")\n",
        "\n",
        "from pyspark.sql.types import FloatType, IntegerType\n",
        "\n",
        "\n",
        "pred = pred.withColumn(\"Id\", pred[\"Id\"].cast(IntegerType()))\n",
        "pred = pred.withColumn(\"SalePrice\", pred[\"SalePrice\"].cast(FloatType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHGYUAaPCrJt"
      },
      "source": [
        "pred_pd = pred.toPandas()\n",
        "save = pred_pd.to_csv(\"submission.csv\", index=False)\n",
        "save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MpylXEICymp"
      },
      "source": [
        "pred_pd"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}